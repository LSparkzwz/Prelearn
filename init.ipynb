{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "init.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vurXuTaNCHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imports\n",
        "!pip install wikipedia\n",
        "!pip install fasttext\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "\n",
        "import string\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from pprint import pprint as print\n",
        "from gensim.models.fasttext import FastText as fasttext\n",
        "from gensim.test.utils import datapath\n",
        "from google.colab import drive\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import fasttext.util\n",
        "import sklearn as sk\n",
        "from sklearn import svm\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25VnlWxIfHb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initial setup\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "fasttext.util.download_model('it', if_exists='ignore') \n",
        "ft = fasttext.load_model('cc.it.300.bin')\n",
        "\n",
        "wikipedia.set_lang(\"it\")\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03BN0_1dfMty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset init\n",
        "\n",
        "#file consisting of pairs of target and prerequisite concepts (A, B) labelled as follows:\n",
        "#1 if B is a prerequisite of A;\n",
        "#0 in all other cases.\n",
        "df = pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "train = df.sample(frac=0.8,random_state=200) \n",
        "validation = df.drop(train.index)\n",
        "\n",
        "#The Wikipedia page of each concept found in the previous file.\n",
        "#Each Wikipedia page is introduced by a `<doc>` element (with *id* and *url*) \n",
        "#containing the title and the text of the corresponding page.\n",
        "tree = ET.parse('/content/drive/My Drive/dataset.xml')\n",
        "pages = tree.getroot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ghW2hZ8d1ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create dictionary of tokenized documents\n",
        "punct = string.punctuation + '«``»' + \"''\"\n",
        "it_stop_words = nltk.corpus.stopwords.words('italian')\n",
        "stemmer = nltk.stem.snowball.ItalianStemmer(True)\n",
        "\n",
        "doc_dict = {}\n",
        "\n",
        "for content in pages.iter('doc'):\n",
        "  #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "  document = (content.find('title').text  + content.find('text').text).replace(\"'\",\" \")\n",
        "  #tokenized\n",
        "  tokenized = nltk.tokenize.word_tokenize(document, \"italian\")\n",
        "  #no punctuation and lowercase\n",
        "  no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "  #remove stop words\n",
        "  no_stop_words = [x for x in no_punct if x not in it_stop_words]\n",
        "  #stem\n",
        "  #stemmed = [stemmer.stem(i) for i in no_stop_words]\n",
        "  \n",
        "  #add tokenized document\n",
        "  doc_dict[content.find('title').text] = no_stop_words"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}