{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "we_word_frequency.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D67docX3sj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create doc embeddings with fasttext by summing word embeddings weighted by word frequency\n",
        "\n",
        "#create train dataset\n",
        "\n",
        "ft_train = {k: [] for k in range(600)}\n",
        "ft_train['prerequisite'] = []\n",
        "for index, row in train.iterrows():\n",
        "    A = doc_dict[row[0]]\n",
        "    B = doc_dict[row[1]]\n",
        "    doc_embedding_A = np.zeros(300)\n",
        "    doc_embedding_B = np.zeros(300)\n",
        "    nA = np.array(A)\n",
        "    nB = np.array(B)\n",
        "\n",
        "    for word in A:\n",
        "      word_embedding = np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_A = doc_embedding_A + word_embedding *  ( np.count_nonzero(nA == word) / len(A))\n",
        "    for word in B:\n",
        "      word_embedding = np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_B = doc_embedding_B + word_embedding  *  ( np.count_nonzero(nB == word) / len(B))\n",
        "    \n",
        "    data = np.concatenate([doc_embedding_A, doc_embedding_B]) \n",
        "    for i,val in enumerate(data):\n",
        "      ft_train[i].append(val)\n",
        "    ft_train['prerequisite'].append(row[2])\n",
        "\n",
        "#create validation dataset\n",
        "\n",
        "ft_validation = {k: [] for k in range(600)}\n",
        "ft_validation['prerequisite'] = []\n",
        "for index, row in validation.iterrows():\n",
        "    A = doc_dict[row[0]]\n",
        "    B = doc_dict[row[1]]\n",
        "    doc_embedding_A = np.zeros(300)\n",
        "    doc_embedding_B = np.zeros(300)\n",
        "    nA = np.array(A)\n",
        "    nB = np.array(B)\n",
        "\n",
        "    for word in A:\n",
        "      word_embedding = np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_A = doc_embedding_A + word_embedding *  ( np.count_nonzero(nA == word) / len(A))\n",
        "    for word in B:\n",
        "      word_embedding = np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_B = doc_embedding_B + word_embedding  *  ( np.count_nonzero(nB == word) / len(B))  \n",
        "\n",
        "    data = np.concatenate([doc_embedding_A, doc_embedding_B]) \n",
        "    for i,val in enumerate(data):\n",
        "      ft_validation[i].append(val)\n",
        "    ft_validation['prerequisite'].append(row[2])\n",
        "\n",
        "ft_df_train = pd.DataFrame(data = ft_train)\n",
        "ft_df_validation = pd.DataFrame(data = ft_validation)\n",
        "\n",
        "y_train = ft_df_train.iloc[:,600]\n",
        "X_train = ft_df_train.iloc[:,:600]\n",
        "\n",
        "X_test = ft_df_validation.iloc[:,:600]\n",
        "y_test = ft_df_validation.iloc[:,600]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}