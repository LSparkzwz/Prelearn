{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "we_biased_A.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdl3WAp5aFcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create doc embeddings with fasttext by averaging word embeddings\n",
        "#but with a higher weight to the words that make up the title in B\n",
        "\n",
        "#intuition:\n",
        "#if I find in A the words that make up the title in B, then there's\n",
        "#a good chance that B is a prerequisite of A\n",
        "\n",
        "#examples of intuition:\n",
        "#the wikipedia page of \"Light\" is a prerequisite of the wikipedia page of \"Total internal reflection\"\n",
        "#the wikipedia page talking about \"Total internal reflection\" mentions the word \"light\" many times\n",
        "#\"Magnet\" is a prerequisite of \"Magnetic field\" \n",
        "#the word \"magnet\" appears various times in the \"Magnetic field\" page\n",
        "\n",
        "#the weight given to the word depends on the length of the document A\n",
        "#the bigger the document the bigger the weight\n",
        "#this is to prevent small documents from being too dependant on the word\n",
        "#and for the word to be irrelevant in big documents\n",
        "\n",
        "word_percent = 0.1\n",
        "\n",
        "#create train dataset\n",
        "\n",
        "ft_train = {k: [] for k in range(600)}\n",
        "ft_train['prerequisite'] = []\n",
        "for index, row in train.iterrows():\n",
        "    A = doc_dict[row[0]]\n",
        "    B_title = wiki_wiki.page(row[1]).langlinks['en'].title\n",
        "    B = doc_dict[row[1]]\n",
        "    doc_embedding_A = np.zeros(300)\n",
        "    doc_embedding_B = np.zeros(300)\n",
        "\n",
        "    #clean the title\n",
        "    B_title = B_title.replace(\"'\",\" \")\n",
        "    #tokenized\n",
        "    B_title_tokenized = nltk.tokenize.word_tokenize(B_title, \"italian\")\n",
        "    #no punctuation and lowercase\n",
        "    B_title_tokenized_no_punct = [x.lower() for x in B_title_tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    B_title = [x for x in B_title_tokenized_no_punct if x not in en_stop_words]\n",
        "\n",
        "    for word in A:\n",
        "      weight = 1\n",
        "      if word in B_title:\n",
        "        #weight = 1\n",
        "        weight = len(A) * word_percent\n",
        "      word_embedding = weight * np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_A = doc_embedding_A + word_embedding\n",
        "\n",
        "    for word in B:\n",
        "      weight = 1\n",
        "        #weight = len(B) * word_percent\n",
        "      word_embedding = weight * np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_B = doc_embedding_B + word_embedding\n",
        "\n",
        "    data = np.concatenate([doc_embedding_A,doc_embedding_B]) \n",
        "    for i,val in enumerate(data):\n",
        "      ft_train[i].append(val)\n",
        "    ft_train['prerequisite'].append(row[2])\n",
        "\n",
        "#create validation dataset\n",
        "\n",
        "ft_validation = {k: [] for k in range(600)}\n",
        "ft_validation['prerequisite'] = []\n",
        "for index, row in validation.iterrows():\n",
        "    A = doc_dict[row[0]]\n",
        "    B_title = wiki_wiki.page(row[1]).langlinks['en'].title\n",
        "    B = doc_dict[row[1]]\n",
        "    doc_embedding_A = np.zeros(300)\n",
        "    doc_embedding_B = np.zeros(300)\n",
        "\n",
        "    #clean the title\n",
        "    B_title = B_title.replace(\"'\",\" \")\n",
        "    #tokenized\n",
        "    B_title_tokenized = nltk.tokenize.word_tokenize(B_title, \"italian\")\n",
        "    #no punctuation and lowercase\n",
        "    B_title_tokenized_no_punct = [x.lower() for x in B_title_tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    B_title = [x for x in B_title_tokenized_no_punct if x not in en_stop_words]\n",
        "\n",
        "    for word in A:\n",
        "      weight = 1\n",
        "      if word in B_title:\n",
        "        #weight = 1\n",
        "        weight = len(A) * word_percent\n",
        "      word_embedding = weight * np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_A = doc_embedding_A + word_embedding\n",
        "\n",
        "    for word in B:\n",
        "      weight = 1\n",
        "      word_embedding = weight * np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_B = doc_embedding_B + word_embedding  \n",
        "\n",
        "    data = np.concatenate([doc_embedding_A,doc_embedding_B]) \n",
        "    for i,val in enumerate(data):\n",
        "      ft_validation[i].append(val)\n",
        "    ft_validation['prerequisite'].append(row[2])\n",
        "\n",
        "ft_df_train = pd.DataFrame(data = ft_train)\n",
        "ft_df_validation = pd.DataFrame(data = ft_validation)\n",
        "\n",
        "y_train = ft_df_train.iloc[:,600]\n",
        "X_train = ft_df_train.iloc[:,:600]\n",
        "\n",
        "X_test = ft_df_validation.iloc[:,:600]\n",
        "y_test = ft_df_validation.iloc[:,600]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}