{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_init_en.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "plEv2yhZ6U5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load all the datasets\n",
        "cs_positives =  pd.read_csv('/content/drive/My Drive/RefD-dataset-master/Course/CS.edges', sep='\\t', header=None)\n",
        "cs_negatives =  pd.read_csv('/content/drive/My Drive/RefD-dataset-master/Course/CS.edges_neg', sep='\\t', header=None)\n",
        "math_positives =  pd.read_csv('/content/drive/My Drive/RefD-dataset-master/Course/MATH.edges', sep='\\t', header=None)\n",
        "math_negatives =  pd.read_csv('/content/drive/My Drive/RefD-dataset-master/Course/MATH.edges_neg', sep='\\t', header=None)\n",
        "#unite the positive datasets\n",
        "#the structure is A,B where B is prereq of A\n",
        "positives = pd.concat([cs_positives,math_positives])\n",
        "#we obtain the negative counterpart of the positive dataset because:\n",
        "#if B is prereq of A, then A is not prereq of B\n",
        "cols = list(positives.columns)\n",
        "cols = cols[1::2] + cols[::2]\n",
        "neg = positives[cols]\n",
        "#now we concat it to the other negative datasets\n",
        "negatives = pd.concat([neg,cs_negatives,math_negatives])\n",
        "#now we add a column with only 1s to the positive dataset and a column of only 0s to the negative one\n",
        "#this way the dataset follows the same standard as the Prelearn one\n",
        "positives[2] = '1'\n",
        "negatives[2] = '0'\n",
        "#finally we concat the two datasets so we only have one\n",
        "df = pd.concat([positives,negatives])\n",
        "#we shuffle the dataframe since right now it would give trouble while creating a proper training and validation set\n",
        "df = df.sample(frac=1)\n",
        "#train and validatin split\n",
        "train = df.sample(frac=0.8,random_state=200) \n",
        "validation = df.drop(train.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDA0gtnX43wp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cb75c1ef-4776-4bde-ee19-6c00558b165d"
      },
      "source": [
        "wiki_wiki = wikipediaapi.Wikipedia('en',extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
        "\n",
        "#create dictionary of tokenized documents\n",
        "punct = string.punctuation + '«``»' + \"''\"\n",
        "en_stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "doc_dict = {}\n",
        "\n",
        "#we get the english wikipedia pages\n",
        "\n",
        "for index,row in df.iterrows():\n",
        "  #Titles of the two documents\n",
        "  title_A = row[0]\n",
        "  title_B = row[1]\n",
        "  \n",
        "  #we check if the page related to the titles is already in the dictionary\n",
        "  if title_A not in doc_dict:\n",
        "    p_wiki = wiki_wiki.page(title_A)\n",
        "    document = (p_wiki.title + \" \" + p_wiki.text).replace(\"'\",\" \")\n",
        "    #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "    document = (p_wiki.title  + p_wiki.text).replace(\"'\",\" \")\n",
        "    document = document.replace(\"\\\\\",\" \")\n",
        "    document = document.replace(\"displaystyle\",\" \")\n",
        "    #tokenized\n",
        "    tokenized = nltk.tokenize.word_tokenize(document, \"english\")\n",
        "    #no punctuation and lowercase\n",
        "    no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    no_stop_words = [x for x in no_punct if x not in en_stop_words]\n",
        "    #add tokenized document\n",
        "    doc_dict[title_A] = no_stop_words\n",
        "\n",
        "  if title_B not in doc_dict:\n",
        "    p_wiki = wiki_wiki.page(title_B)\n",
        "    document = (p_wiki.title + \" \" + p_wiki.text).replace(\"'\",\" \")\n",
        "    #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "    document = (p_wiki.title  + p_wiki.text).replace(\"'\",\" \")\n",
        "    document = document.replace(\"\\\\\",\" \")\n",
        "    document = document.replace(\"displaystyle\",\" \")\n",
        "    #tokenized\n",
        "    tokenized = nltk.tokenize.word_tokenize(document, \"english\")\n",
        "    #no punctuation and lowercase\n",
        "    no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    no_stop_words = [x for x in no_punct if x not in en_stop_words]\n",
        "    #add tokenized document\n",
        "    doc_dict[title_B] = no_stop_words"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['Software metric', 'Information management', 'Software architecture', 'Software engineering', 'Internet protocol suite', 'Network service', 'Computer-aided software engineering', 'Cloud computing', 'Computer security', 'Theory of computation', 'Computer architecture', 'Operating system', 'Discrete mathematics', 'Algebra', 'Object-oriented design', 'Data structure', 'Lie theory', 'Group theory', 'Ordinary differential equation', 'Vector calculus', 'Mathematical logic', 'Algebraic number theory', 'Software project management', 'Cryptography', 'Linear algebra', 'Network security', 'Translator (computing)', 'Set theory', 'Differential equation', 'Fourier analysis', 'Dynamical system', 'Software testing', 'Game theory', 'Algorithm', 'System programming', 'Software development', 'General relativity', 'Approximation theory', 'Machine learning', 'Information security', 'Artificial intelligence', 'Combinatorics', 'Mathematical analysis', 'Mathematical model', 'Fourier series', 'Distributed computing', 'Assembly language', 'Broadband networks', 'Combinatorial optimization', 'Abstract algebra', 'Object-oriented programming', 'Von Neumann algebra', 'Matrix (mathematics)', 'Data mining', 'Algebraic topology', 'Mathematical statistics', 'Commutative algebra', 'Knowledge management', 'Robotics', 'Special relativity', 'K-theory', 'Wavelet', 'Partial differential equation', 'Computability theory', 'Data integration', 'Topology', 'Probability theory', 'Wireless network', 'Algorithm design', 'Information retrieval', 'Orthogonal polynomials', 'Algebraic geometry', 'Coding theory', 'Computer network', 'Topological space', 'Analytic geometry', 'Differentiable manifold', 'Parallel processing', 'Linear programming', 'Trigonometry', 'Calculus', 'Object-oriented analysis and design', 'Natural language processing', 'Analytical mechanics', 'Software analytics', 'Primality test', 'Operations research', 'Computer graphics', 'Test automation', 'Complex analysis', 'Numerical analysis', 'Number theory', 'Functional analysis', 'Real analysis', 'Differential geometry', 'C*-algebra', 'Stochastic process', 'Microarchitecture', 'Mobile application development', 'Parallel computing', 'Analytic number theory', 'Database', 'Social network analysis', 'Programming language', 'Ergodic theory', 'Distributed object', 'Multivariable calculus', 'Finite element method', 'Computational linguistics', 'Numerical linear algebra', 'Software quality management', 'Compiler construction', 'Graphical model', 'Data warehouse', 'Metric (mathematics)', 'Data-intensive computing', 'Computational geometry', 'Computer vision', 'Field (mathematics)', 'Mathematical optimization', 'Statistics', 'Analysis of algorithms', 'Factorization', 'Modeling language', 'Differential topology', 'Multiscale modeling', 'Graph theory'])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
