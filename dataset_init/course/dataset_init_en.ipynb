{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_init_en.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y_Pogg3hj_h"
      },
      "source": [
        "#load all the datasets\n",
        "cs_positives =  pd.read_csv('/content/drive/My Drive/RefD-dataset-master/Course/CS.edges', sep='\\t', header=None)\n",
        "cs_negatives =  pd.read_csv('/content/drive/My Drive/RefD-dataset-master/Course/CS.edges_neg', sep='\\t', header=None)\n",
        "math_positives =  pd.read_csv('/content/drive/My Drive/RefD-dataset-master/Course/MATH.edges', sep='\\t', header=None)\n",
        "math_negatives =  pd.read_csv('/content/drive/My Drive/RefD-dataset-master/Course/MATH.edges_neg', sep='\\t', header=None)\n",
        "#unite the positive datasets\n",
        "#the structure is A,B where B is prereq of A\n",
        "positives = pd.concat([cs_positives,math_positives])\n",
        "#we obtain the negative counterpart of the positive dataset because:\n",
        "#if B is prereq of A, then A is not prereq of B\n",
        "cols = list(positives.columns)\n",
        "cols = cols[1::2] + cols[::2]\n",
        "neg = positives[cols]\n",
        "#now we concat it to the other negative datasets\n",
        "negatives = pd.concat([neg,cs_negatives,math_negatives])\n",
        "#now we add a column with only 1s to the positive dataset and a column of only 0s to the negative one\n",
        "#this way the dataset follows the same standard as the Prelearn one\n",
        "positives[2] = 1\n",
        "negatives[2] = 0\n",
        "\n",
        "#We create the train and validation dataset like this because we have very few positives compared to the negatives\n",
        "#so we spread them more fairly\n",
        "positives_train = positives.sample(frac=0.8,random_state=200) \n",
        "positives_validation = positives.drop(positives_train.index)\n",
        "negatives_train = negatives.sample(frac=0.8,random_state=200) \n",
        "negatives_validation = negatives.drop(negatives_train.index)\n",
        "train = pd.concat([positives_train,negatives_train])\n",
        "validation = pd.concat([positives_validation,negatives_validation])\n",
        "#we shuffle the dataframe to create randomness\n",
        "train = train.sample(frac=1)\n",
        "validation = validation.sample(frac=1)\n",
        "\n",
        "#df is needed for later, it's the whole dataset\n",
        "df = pd.concat([positives,negatives])\n",
        "#we shuffle the dataframe since right now it would give trouble while creating a proper training and validation set\n",
        "df = df.sample(frac=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5g8CcjThpAt"
      },
      "source": [
        "wiki_wiki = wikipediaapi.Wikipedia('en',extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
        "\n",
        "#create dictionary of tokenized documents\n",
        "punct = string.punctuation + '«``»' + \"''\"\n",
        "en_stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "doc_dict = {}\n",
        "\n",
        "#we get the english wikipedia pages\n",
        "\n",
        "for index,row in df.iterrows():\n",
        "  #Titles of the two documents\n",
        "  title_A = row[0]\n",
        "  title_B = row[1]\n",
        "  \n",
        "  #we check if the page related to the titles is already in the dictionary\n",
        "  if title_A not in doc_dict:\n",
        "    p_wiki = wiki_wiki.page(title_A)\n",
        "    document = (p_wiki.title + \" \" + p_wiki.text).replace(\"'\",\" \")\n",
        "    #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "    document = document.replace(\"\\\\\",\" \")\n",
        "    document = document.replace(\"displaystyle\",\" \")\n",
        "    #tokenized\n",
        "    tokenized = nltk.tokenize.word_tokenize(document, \"english\")\n",
        "    #no punctuation and lowercase\n",
        "    no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    no_stop_words = [x for x in no_punct if x not in en_stop_words]\n",
        "    #add tokenized document\n",
        "    doc_dict[title_A] = no_stop_words\n",
        "\n",
        "  if title_B not in doc_dict:\n",
        "    p_wiki = wiki_wiki.page(title_B)\n",
        "    document = (p_wiki.title + \" \" + p_wiki.text).replace(\"'\",\" \")\n",
        "    #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "    document = document.replace(\"\\\\\",\" \")\n",
        "    document = document.replace(\"displaystyle\",\" \")\n",
        "    #tokenized\n",
        "    tokenized = nltk.tokenize.word_tokenize(document, \"english\")\n",
        "    #no punctuation and lowercase\n",
        "    no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    no_stop_words = [x for x in no_punct if x not in en_stop_words]\n",
        "    #add tokenized document\n",
        "    doc_dict[title_B] = no_stop_words\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
