{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_init_it.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "03BN0_1dfMty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wikipedia.set_lang(\"it\")\n",
        "\n",
        "#dataset init\n",
        "\n",
        "#file consisting of pairs of target and prerequisite concepts (A, B) labelled as follows:\n",
        "#1 if B is a prerequisite of A;\n",
        "#0 in all other cases.\n",
        "df = pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "train = df.sample(frac=0.8,random_state=200) \n",
        "validation = df.drop(train.index)\n",
        "\n",
        "#The Wikipedia page of each concept found in the previous file.\n",
        "#Each Wikipedia page is introduced by a `<doc>` element (with *id* and *url*) \n",
        "#containing the title and the text of the corresponding page.\n",
        "tree = ET.parse('/content/drive/My Drive/dataset.xml')\n",
        "pages = tree.getroot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ghW2hZ8d1ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create dictionary of tokenized documents\n",
        "punct = string.punctuation + '«``»' + \"''\"\n",
        "it_stop_words = nltk.corpus.stopwords.words('italian')\n",
        "stemmer = nltk.stem.snowball.ItalianStemmer(True)\n",
        "\n",
        "doc_dict = {}\n",
        "\n",
        "for content in pages.iter('doc'):\n",
        "  #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "  document = (content.find('title').text  + content.find('text').text).replace(\"'\",\" \")\n",
        "  #tokenized\n",
        "  tokenized = nltk.tokenize.word_tokenize(document, \"italian\")\n",
        "  #no punctuation and lowercase\n",
        "  no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "  #remove stop words\n",
        "  no_stop_words = [x for x in no_punct if x not in it_stop_words]\n",
        "  #stem\n",
        "  #stemmed = [stemmer.stem(i) for i in no_stop_words]\n",
        "  \n",
        "  #add tokenized document\n",
        "  doc_dict[content.find('title').text] = no_stop_words"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}