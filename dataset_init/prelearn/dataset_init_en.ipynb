{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_init_en.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "03BN0_1dfMty"
      },
      "source": [
        "wiki_wiki = wikipediaapi.Wikipedia('it',extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
        "#This is quick fix to make the word embeddings parametric\n",
        "#The problem is that the Prelearn dataset is Italian and it relies on Wikipedia APIs to get the English equivalent\n",
        "#Therefore the embeddings need those APIs, but they're not needed by the other datasets\n",
        "#I could translated the dataset here but I tried and it takes waaaaay too much\n",
        "is_Prelearn = True\n",
        "\n",
        "#dataset init\n",
        "\n",
        "#file consisting of pairs of target and prerequisite concepts (A, B) labelled as follows:\n",
        "#1 if B is a prerequisite of A;\n",
        "#0 in all other cases.\n",
        "df = pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "train = df.sample(frac=0.8,random_state=200) \n",
        "validation = df.drop(train.index)\n",
        "\n",
        "#The Wikipedia page of each concept found in the previous file.\n",
        "#Each Wikipedia page is introduced by a `<doc>` element (with *id* and *url*) \n",
        "#containing the title and the text of the corresponding page.\n",
        "\n",
        "#THE PAGES HERE ARE IN ITALIAN\n",
        "#Since we want the English equivalent, we will only get the page title, and then use the Wikipedia api to get its translation\n",
        "\n",
        "tree = ET.parse('/content/drive/My Drive/dataset.xml')\n",
        "pages = tree.getroot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ghW2hZ8d1ip"
      },
      "source": [
        "#create dictionary of tokenized documents\n",
        "punct = string.punctuation + '«``»' + \"''\"\n",
        "en_stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "doc_dict = {}\n",
        "\n",
        "#we get the english wikipedia pages\n",
        "\n",
        "for content in pages.iter('doc'):\n",
        "  #Italian title\n",
        "  title = content.find('title').text\n",
        "  #Eng page\n",
        "  p_wiki = wiki_wiki.page(title).langlinks['en']\n",
        "  #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "  document = (p_wiki.title + \" \"  + p_wiki.text).replace(\"'\",\" \")\n",
        "  document = document.replace(\"\\\\\",\" \")\n",
        "  document = document.replace(\"displaystyle\",\" \")\n",
        "  #tokenized\n",
        "  tokenized = nltk.tokenize.word_tokenize(document, \"english\")\n",
        "  #no punctuation and lowercase\n",
        "  no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "  #remove stop words\n",
        "  no_stop_words = [x for x in no_punct if x not in en_stop_words]\n",
        "\n",
        "    #add tokenized document\n",
        "  doc_dict[content.find('title').text] = no_stop_words"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}