{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "init_en.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqwXn2bdus0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "#the prerequisites relationship dataset in this case is a matrix\n",
        "#  B  B  B ...\n",
        "#A 0  1  0\n",
        "#A 0  0  1\n",
        "#A 0  0  0\n",
        "#..\n",
        "#we don't really have 0 and 1, we have a number of votes \n",
        "#the votes are given by 4 annotators\n",
        "#vote = \"I think there's a prerequisite relationship\"\n",
        "#in this scenario we consider 2+ votes as 1, everything else as 0\n",
        "\n",
        "#the dataframe we create\n",
        "#              A   B   0/1\n",
        "df = {'A':[],'B':[],'prerequisite':[]}\n",
        "B_list = []\n",
        "\n",
        "with open('/content/drive/My Drive/datasets/matrix_with_annotators_names.csv') as csv_file:\n",
        "    #the first row is the header containing the B elements\n",
        "    #we save it to know the relationships\n",
        "    B_list = []\n",
        "    is_first_line = True\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_reader:\n",
        "      if is_first_line:\n",
        "        B_list = row\n",
        "        is_first_line = False\n",
        "      else:\n",
        "        #the first element is the element A\n",
        "        #every other row is the prerequisite relationship with B\n",
        "        #so we skip the first element\n",
        "        A = row[0]\n",
        "        row = iter(row)\n",
        "        next(row)\n",
        "        for index, element in enumerate(row):\n",
        "          #as said before, +2 votes = 1\n",
        "          #else = 0\n",
        "          df['A'].append(A)\n",
        "          #index+1 because we skipped the first element\n",
        "          df['B'].append(B_list[index+1])\n",
        "          if len(element.split(\" \")) >= 2:\n",
        "            df['prerequisite'].append(1)\n",
        "          else:\n",
        "            df['prerequisite'].append(0)\n",
        "\n",
        "df = pd.DataFrame.from_dict(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK7aOndNlMFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create dictionary of tokenized documents\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "punct = string.punctuation + '«``»' + \"''\"\n",
        "en_stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "doc_dict = {}\n",
        "#this is the text file with the context information for each term\n",
        "#we don't have wikipedia pages or something similar\n",
        "#just this text file\n",
        "#so we will get the 10 words around each term to get a context\n",
        "#we get a context every time we find the term inside the file\n",
        "#so if a term appears multiple times we get the context multiple times so there's more information\n",
        "page_file = open('/content/drive/My Drive/datasets/cap4_annotato_PRET.txt', \"r\")\n",
        "page = page_file.read()\n",
        "page_file.close()\n",
        "#we clean the text file \n",
        "page = re.sub(r'<.+?>', ' ', page)\n",
        "page = page.replace(\"#\",\" \")\n",
        "page = page.replace(\"@\",\" \")\n",
        "#we split the string into an array of words\n",
        "#since the word titles are composed by multiple words divided by space\n",
        "#we first divide the string so that those words are considered a single word\n",
        "\n",
        "\n",
        "for index,row in df.iterrows():\n",
        "  #Titles of the two documents\n",
        "  title_A = row[0]\n",
        "  title_B = row[1]\n",
        "  \n",
        "  #we check if the page related to the titles is already in the dictionary\n",
        "  if title_A not in doc_dict:\n",
        "    #find the 10 words around title_A\n",
        "    document = \"\"\n",
        "    #we get the ten words around every match\n",
        "    #there's a problem with this\n",
        "    #some terms are substrings of other terms\n",
        "    #for example COMPUTER is a substring of COMPUTER NETWORKS\n",
        "    #this means COMPUTER will match with the COMPUTER of COMPUTER NETWORKS\n",
        "    #I don't have the time to solve this problem\n",
        "    for match in re.finditer(title_A, page):\n",
        "      before = page[:match.start()].split()[15:]\n",
        "      after = page[match.start():].split()[:15]\n",
        "      before = ' '.join([str(elem) for elem in before]) \n",
        "      after = ' '.join([str(elem) for elem in after]) \n",
        "      document = document + before + after\n",
        "    #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "    document = document.replace(\"s\",\"\")\n",
        "    document = document.replace(\"'\",\" \")\n",
        "    document = document.replace(\"\\\\\",\" \")\n",
        "    #tokenized\n",
        "    tokenized = nltk.tokenize.word_tokenize(document, \"english\")\n",
        "    #no punctuation and lowercase\n",
        "    no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    no_stop_words = [x for x in no_punct if x not in en_stop_words]\n",
        "    #add tokenized document\n",
        "    doc_dict[title_A] = no_stop_words\n",
        "\n",
        "  if title_B not in doc_dict:\n",
        "    #find the 10 words around title_B\n",
        "    document = \"\"\n",
        "    #we get the ten words around every match\n",
        "    #there's a problem with this\n",
        "    #some terms are substrings of other terms\n",
        "    #for example COMPUTER is a substring of COMPUTER NETWORKS\n",
        "    #this means COMPUTER will match with the COMPUTER of COMPUTER NETWORKS\n",
        "    #I don't have the time to solve this problem\n",
        "    for match in re.finditer(title_B, page):\n",
        "      before = page[:match.start()].split()[10:]\n",
        "      after = page[match.start():].split()[:10]\n",
        "      before = ' '.join([str(elem) for elem in before]) \n",
        "      after = ' '.join([str(elem) for elem in after]) \n",
        "      document = document + before + after\n",
        "    #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "    document = document.replace(\"'\",\" \")\n",
        "    document = document.replace(\"\\\\\",\" \")\n",
        "    #tokenized\n",
        "    tokenized = nltk.tokenize.word_tokenize(document, \"english\")\n",
        "    #no punctuation and lowercase\n",
        "    no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    no_stop_words = [x for x in no_punct if x not in en_stop_words]\n",
        "    #add tokenized document\n",
        "    doc_dict[title_B] = no_stop_words "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}