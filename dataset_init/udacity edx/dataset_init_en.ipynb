{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "init_en.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NBpgzvFaHu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "#we first get the absolute paths of every positive and negative datasets so we can then read them\n",
        "\n",
        "#edX dataset\n",
        "#find all the positive datasets\n",
        "edx_positives  = os.popen(\"find ./drive/My\\ Drive/edX-Udacity-dataset/edX-prerequisites/edX-prerequisites -name 'positive.txt'\").read().split('\\n')\n",
        "#find all the negative datasets\n",
        "edx_negatives = os.popen(\"find ./drive/My\\ Drive/edX-Udacity-dataset/edX-prerequisites/edX-prerequisites -name 'negative.txt'\").read().split('\\n')\n",
        "\n",
        "#Udacity\n",
        "#find all the positive datasets\n",
        "udacity_positives = os.popen(\"find ./drive/My\\ Drive/edX-Udacity-dataset/Udacity-prerequisites/Udacity-prerequisites -name 'positive.txt'\").read().split('\\n')\n",
        "#find all the negative datasets\n",
        "udacity_negatives = os.popen(\"find ./drive/My\\ Drive/edX-Udacity-dataset/Udacity-prerequisites/Udacity-prerequisites -name 'negative.txt'\").read().split('\\n')\n",
        "\n",
        "#list of all the absolute paths of the positive and negative datasets\n",
        "#we also pop the last element since due to the split done before it's just an empty string\n",
        "positives = edx_positives[:-1] + udacity_positives[:-1]\n",
        "negatives = edx_negatives[:-1] + udacity_negatives[:-1]\n",
        "\n",
        "\n",
        "df_positives = pd.DataFrame() \n",
        "df_negatives = pd.DataFrame() \n",
        "#we now create a dataset for all the positive and negative pairs\n",
        "#the elements of the pair are the filename of the referenced article, not the article itself\n",
        "for path in positives:\n",
        "  temp_df = pd.read_csv(path, sep='\\t', header=None, warn_bad_lines=True, error_bad_lines=False)\n",
        "  df_positives = pd.concat([df_positives,temp_df])\n",
        "\n",
        "for path in negatives:\n",
        "  temp_df = pd.read_csv(path, sep='\\t', header=None, warn_bad_lines=True, error_bad_lines=False)\n",
        "  df_negatives = pd.concat([df_negatives,temp_df])\n",
        "\n",
        "#we obtain the negative counterpart of the positive dataset because:\n",
        "#if B is prereq of A, then A is not prereq of B\n",
        "cols = list(df_positives.columns)\n",
        "cols = cols[1::2] + cols[::2]\n",
        "neg = df_positives[cols]\n",
        "#we concat it to the other negatives dataset\n",
        "df_negatives = pd.concat([df_negatives,neg])\n",
        "\n",
        "#we now assign the third column that says if there's a prerequisite or not\n",
        "#1 for positive\n",
        "#0 for negative\n",
        "df_positives['2'] = '1'\n",
        "df_negatives['2'] = '0'\n",
        "\n",
        "#finally we concat everything into an unique dataset\n",
        "df = pd.concat([df_positives,df_negatives])\n",
        "#we shuffle the dataframe since right now it would give trouble while creating a proper training and validation set\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "#train and validatin split\n",
        "train = df.sample(frac=0.8,random_state=200) \n",
        "validation = df.drop(train.index)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK7aOndNlMFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create dictionary of tokenized documents\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "punct = string.punctuation + '«``»' + \"''\"\n",
        "en_stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "doc_dict = {}\n",
        "\n",
        "#we get the pages related to the filenames in the\n",
        "\n",
        "for index,row in df.iterrows():\n",
        "  #Titles of the two documents\n",
        "  filename_A = row[0]\n",
        "  filename_B = row[1]\n",
        "  \n",
        "  #we check if the page related to the titles is already in the dictionary\n",
        "  if filename_A not in doc_dict:\n",
        "    #we get the path of the page\n",
        "    #there can be multiple results, they're all the same (there are copies of the same file)\n",
        "    #we just get the first\n",
        "    path = os.popen(\"find ./drive/My\\ Drive/edX-Udacity-dataset -name '\" + filename_A + \"'\").read().split('\\n')[0]\n",
        "    # print(\"--------------------\")\n",
        "    # print(os.popen(\"find ./drive/My\\ Drive/edX-Udacity-dataset -name '\" + filename_A + \"'\").read())\n",
        "    # print(filename_A)\n",
        "    page_file = open(path, \"r\")\n",
        "    page = page_file.read()\n",
        "    page_file.close()\n",
        "    #clean the page \n",
        "    #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "    document = page.replace(\"'\",\" \")\n",
        "    document = document.replace(\"\\\\\",\" \")\n",
        "    document = document.replace(\"displaystyle\",\" \")\n",
        "    #tokenized\n",
        "    tokenized = nltk.tokenize.word_tokenize(document, \"english\")\n",
        "    #no punctuation and lowercase\n",
        "    no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    no_stop_words = [x for x in no_punct if x not in en_stop_words]\n",
        "    #add tokenized document\n",
        "    doc_dict[filename_A] = no_stop_words\n",
        "\n",
        "  if filename_B not in doc_dict:\n",
        "    #we get the path of the page\n",
        "    #there can be multiple results, they're all the same (there are copies of the same file)\n",
        "    #we just get the first\n",
        "    path = os.popen(\"find ./drive/My\\ Drive/edX-Udacity-dataset -name '\" + filename_B + \"'\").read().split('\\n')[0]\n",
        "    page_file = open(path, \"r\")\n",
        "    page = page_file.read()\n",
        "    page_file.close()\n",
        "    #clean the page \n",
        "    #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "    document = page.replace(\"'\",\" \")\n",
        "    document = document.replace(\"\\\\\",\" \")\n",
        "    document = document.replace(\"displaystyle\",\" \")\n",
        "    #tokenized\n",
        "    tokenized = nltk.tokenize.word_tokenize(document, \"english\")\n",
        "    #no punctuation and lowercase\n",
        "    no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    no_stop_words = [x for x in no_punct if x not in en_stop_words]\n",
        "    #add tokenized document\n",
        "    doc_dict[filename_B] = no_stop_words "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}