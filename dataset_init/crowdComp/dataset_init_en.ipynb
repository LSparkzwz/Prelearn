{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "init_en.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqwXn2bdus0e"
      },
      "source": [
        "#dataset init\n",
        "\n",
        "#dataset description: https://github.com/harrylclc/RefD-dataset/\n",
        "global_warming = pd.read_csv('/content/drive/My Drive//RefD-dataset-master/CrowdComp/CrowdComp_Global_warming_Prerequisite_HIT.csv', warn_bad_lines=True, error_bad_lines=False)\n",
        "meiosis = pd.read_csv('/content/drive/My Drive//RefD-dataset-master/CrowdComp/CrowdComp_Meiosis_Prerequisite_HIT.csv', warn_bad_lines=True, error_bad_lines=False)\n",
        "newton = pd.read_csv('/content/drive/My Drive//RefD-dataset-master/CrowdComp/CrowdComp_Newton_Laws_Prerequisite_HIT.csv', warn_bad_lines=True, error_bad_lines=False)\n",
        "parallel_postulate = pd.read_csv('/content/drive/My Drive//RefD-dataset-master/CrowdComp/CrowdComp_Parallel_Postulate_Prerequisite_HIT.csv', warn_bad_lines=True, error_bad_lines=False)\n",
        "pk_cryptography = pd.read_csv('/content/drive/My Drive//RefD-dataset-master/CrowdComp/CrowdComp_PublicKeyCryptography_Prerequisite_HIT.csv', warn_bad_lines=True, error_bad_lines=False)\n",
        "#merge the datasets\n",
        "df = pd.concat([global_warming,meiosis,newton,parallel_postulate,pk_cryptography])\n",
        "#each A B pair in this dataset has multiple possible answers because multiple people gave an answer on the same pair\n",
        "#possible answers are:\n",
        "#A is prerequisite of B\n",
        "#B is prerequisite of A\n",
        "#other (unrelated, related but not prereq, don't know, etc)\n",
        "#we will take as true the answer with most votes\n",
        "#on equal amount of answers we consider A and B unrelated\n",
        "#to do this we use a dictionary where the key is A+B \n",
        "#and the value is the number of votes for each answer and the names of A and B\n",
        "#dict[A+B] = [A,B,AB,BA,unrelated]\n",
        "#AB = A is prereq of B \n",
        "#BA = B is prereq of A\n",
        "dict_votes = {}\n",
        "for index,row in df.iterrows():\n",
        "  A = row['Input.SourceConceptBase']\n",
        "  B = row['Input.TargetConceptBase']\n",
        "  key =  A+B\n",
        "  AB = 0\n",
        "  BA = 0\n",
        "  unrelated = 0\n",
        "  if str(row['Answer.Source2Target']) == 'Source2Target':\n",
        "    AB = 1\n",
        "  elif str(row['Answer.Target2Source']) == 'Target2Source':\n",
        "    BA = 1\n",
        "  else:\n",
        "    unrelated = 1  \n",
        "\n",
        "  if key not in dict_votes:\n",
        "    dict_votes[key] = [A,B,AB,BA,unrelated]\n",
        "  else:\n",
        "    dict_votes[key][2] = dict_votes[key][2] + AB\n",
        "    dict_votes[key][3] = dict_votes[key][3] + BA\n",
        "    dict_votes[key][4] = dict_votes[key][4] + unrelated\n",
        "\n",
        "#now we iterate the values of the dictionary and create a dataset A B 0/1\n",
        "dataset = {'A':[],'B':[],'prerequisite':[]}\n",
        "for key, value in dict_votes.items():\n",
        "  A = value[0]\n",
        "  B = value[1]\n",
        "  votes = [value[2],value[3],value[4]]\n",
        "  m = max(votes)\n",
        "  position_of_max_value = [i for i, j in enumerate(votes) if j == m]\n",
        "  #if there's not unique max vote or if the max vote is unrelated\n",
        "  if len(position_of_max_value) > 1 or position_of_max_value[0] == 2:\n",
        "    dataset['A'].append(A)\n",
        "    dataset['B'].append(B)\n",
        "    dataset['prerequisite'].append(0)\n",
        "    dataset['A'].append(B)\n",
        "    dataset['B'].append(A)\n",
        "    dataset['prerequisite'].append(0)\n",
        "  elif position_of_max_value[0] == 0:\n",
        "    dataset['A'].append(A)\n",
        "    dataset['B'].append(B)\n",
        "    dataset['prerequisite'].append(1)\n",
        "    dataset['A'].append(B)\n",
        "    dataset['B'].append(A)\n",
        "    dataset['prerequisite'].append(0)\n",
        "  elif position_of_max_value[0] == 1:\n",
        "    dataset['A'].append(A)\n",
        "    dataset['B'].append(B)\n",
        "    dataset['prerequisite'].append(0)\n",
        "    dataset['A'].append(B)\n",
        "    dataset['B'].append(A)\n",
        "    dataset['prerequisite'].append(1)\n",
        "\n",
        "df = pd.DataFrame.from_dict(dataset) \n",
        "#we shuffle the dataframe since right now it would give trouble while creating a proper training and validation set\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "#train and validatin split\n",
        "train = df.sample(frac=0.8,random_state=200) \n",
        "validation = df.drop(train.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK7aOndNlMFa"
      },
      "source": [
        "wiki_wiki = wikipediaapi.Wikipedia('en',extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
        "\n",
        "#create dictionary of tokenized documents\n",
        "punct = string.punctuation + '«``»' + \"''\"\n",
        "en_stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "doc_dict = {}\n",
        "\n",
        "#we get the english wikipedia pages\n",
        "\n",
        "for index,row in df.iterrows():\n",
        "  #Titles of the two documents\n",
        "  title_A = row[0]\n",
        "  title_B = row[1]\n",
        "  \n",
        "  #we check if the page related to the titles is already in the dictionary\n",
        "  if title_A not in doc_dict:\n",
        "    p_wiki = wiki_wiki.page(title_A)\n",
        "    document = (p_wiki.title + \" \" + p_wiki.text).replace(\"'\",\" \")\n",
        "    #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "    document = (p_wiki.title  + p_wiki.text).replace(\"'\",\" \")\n",
        "    document = document.replace(\"\\\\\",\" \")\n",
        "    document = document.replace(\"displaystyle\",\" \")\n",
        "    #tokenized\n",
        "    tokenized = nltk.tokenize.word_tokenize(document, \"english\")\n",
        "    #no punctuation and lowercase\n",
        "    no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    no_stop_words = [x for x in no_punct if x not in en_stop_words]\n",
        "    #add tokenized document\n",
        "    doc_dict[title_A] = no_stop_words\n",
        "\n",
        "  if title_B not in doc_dict:\n",
        "    p_wiki = wiki_wiki.page(title_B)\n",
        "    document = (p_wiki.title + \" \" + p_wiki.text).replace(\"'\",\" \")\n",
        "    #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "    document = (p_wiki.title  + p_wiki.text).replace(\"'\",\" \")\n",
        "    document = document.replace(\"\\\\\",\" \")\n",
        "    document = document.replace(\"displaystyle\",\" \")\n",
        "    #tokenized\n",
        "    tokenized = nltk.tokenize.word_tokenize(document, \"english\")\n",
        "    #no punctuation and lowercase\n",
        "    no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    no_stop_words = [x for x in no_punct if x not in en_stop_words]\n",
        "    #add tokenized document\n",
        "    doc_dict[title_B] = no_stop_words\n",
        "\n",
        "print(doc_dict)   "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}