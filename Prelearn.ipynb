{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vurXuTaNCHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "outputId": "fd142296-886c-4559-c4b8-7cc2298388ec"
      },
      "source": [
        "#imports\n",
        "!pip install wikipedia\n",
        "!pip install fasttext\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "\n",
        "import string\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from pprint import pprint as print\n",
        "from gensim.models.fasttext import FastText as fasttext\n",
        "from gensim.test.utils import datapath\n",
        "from google.colab import drive\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import fasttext.util\n",
        "import sklearn as sk\n",
        "from sklearn import svm\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.6.20)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=5e2608a6cada8a024ec98b77af002d1d82f12cdb15699f041c4db317222de7ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (49.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3019109 sha256=89f4e80d1582f2365dbe90c143fa598b57da65cdeef8b3d51905eb8cd2691108\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.48)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.48)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25VnlWxIfHb3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "65f917d0-076d-4bc3-e7bc-bd9154e317cd"
      },
      "source": [
        "#initial setup\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "fasttext.util.download_model('it', if_exists='ignore') \n",
        "ft = fasttext.load_model('cc.it.300.bin')\n",
        "\n",
        "wikipedia.set_lang(\"it\")\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.it.300.bin.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03BN0_1dfMty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset init\n",
        "\n",
        "#file consisting of pairs of target and prerequisite concepts (A, B) labelled as follows:\n",
        "#1 if B is a prerequisite of A;\n",
        "#0 in all other cases.\n",
        "df = pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "train = df.sample(frac=0.8,random_state=200) \n",
        "validation = df.drop(train.index)\n",
        "\n",
        "#The Wikipedia page of each concept found in the previous file.\n",
        "#Each Wikipedia page is introduced by a `<doc>` element (with *id* and *url*) \n",
        "#containing the title and the text of the corresponding page.\n",
        "tree = ET.parse('/content/drive/My Drive/dataset.xml')\n",
        "pages = tree.getroot()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ghW2hZ8d1ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create dictionary of tokenized documents\n",
        "punct = string.punctuation + '«``»' + \"''\"\n",
        "it_stop_words = nltk.corpus.stopwords.words('italian')\n",
        "stemmer = nltk.stem.snowball.ItalianStemmer(True)\n",
        "\n",
        "doc_dict = {}\n",
        "\n",
        "for content in pages.iter('doc'):\n",
        "  #document to lowercase, replaced apostrophe with space since the tokenizer isn't able to split words like \"l'addizione\"\n",
        "  document = (content.find('title').text  + content.find('text').text).replace(\"'\",\" \")\n",
        "  #tokenized\n",
        "  tokenized = nltk.tokenize.word_tokenize(document, \"italian\")\n",
        "  #no punctuation and lowercase\n",
        "  no_punct = [x.lower() for x in tokenized if x not in punct]\n",
        "  #remove stop words\n",
        "  no_stop_words = [x for x in no_punct if x not in it_stop_words]\n",
        "  #stem\n",
        "  #stemmed = [stemmer.stem(i) for i in no_stop_words]\n",
        "  \n",
        "  #add tokenized document\n",
        "  doc_dict[content.find('title').text] = no_stop_words"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNa2mg45Be3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create doc embeddings with fasttext by averaging word embeddings\n",
        "weight_A = 1\n",
        "weight_B = 1\n",
        "\n",
        "#create train dataset\n",
        "\n",
        "ft_train = {k: [] for k in range(600)}\n",
        "ft_train['prerequisite'] = []\n",
        "for index, row in train.iterrows():\n",
        "    A = doc_dict[row[0]]\n",
        "    B = doc_dict[row[1]]\n",
        "    doc_embedding_A = np.zeros(300)\n",
        "    doc_embedding_B = np.zeros(300)\n",
        "\n",
        "    for word in A:\n",
        "      word_embedding = np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_A = doc_embedding_A + word_embedding\n",
        "    for word in B:\n",
        "      word_embedding = np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_B = doc_embedding_B + word_embedding  \n",
        "    \n",
        "    #averaged doc_embeddings\n",
        "    doc_embedding_A = doc_embedding_A / len(A)\n",
        "    doc_embedding_B = doc_embedding_B / len(B)\n",
        "\n",
        "    data = np.concatenate([weight_A * doc_embedding_A, weight_B * doc_embedding_B]) \n",
        "    for i,val in enumerate(data):\n",
        "      ft_train[i].append(val)\n",
        "    ft_train['prerequisite'].append(row[2])\n",
        "\n",
        "#create validation dataset\n",
        "\n",
        "ft_validation = {k: [] for k in range(600)}\n",
        "ft_validation['prerequisite'] = []\n",
        "for index, row in validation.iterrows():\n",
        "    A = doc_dict[row[0]]\n",
        "    B = doc_dict[row[1]]\n",
        "    doc_embedding_A = np.zeros(300)\n",
        "    doc_embedding_B = np.zeros(300)\n",
        "\n",
        "    for word in A:\n",
        "      word_embedding = np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_A = doc_embedding_A + word_embedding\n",
        "    for word in B:\n",
        "      word_embedding = np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_B = doc_embedding_B + word_embedding  \n",
        "    \n",
        "    #averaged doc_embeddings\n",
        "    doc_embedding_A = doc_embedding_A / len(A)\n",
        "    doc_embedding_B = doc_embedding_B / len(B)\n",
        "\n",
        "    data = np.concatenate([weight_A * doc_embedding_A, weight_B * doc_embedding_B]) \n",
        "    for i,val in enumerate(data):\n",
        "      ft_validation[i].append(val)\n",
        "    ft_validation['prerequisite'].append(row[2])\n",
        "\n",
        "ft_df_train = pd.DataFrame(data = ft_train)\n",
        "ft_df_validation = pd.DataFrame(data = ft_validation)\n",
        "\n",
        "y_train = ft_df_train.iloc[:,600]\n",
        "X_train = ft_df_train.iloc[:,:600]\n",
        "\n",
        "X_test = ft_df_validation.iloc[:,:600]\n",
        "y_test = ft_df_validation.iloc[:,600]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdl3WAp5aFcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create doc embeddings with fasttext by averaging word embeddings\n",
        "#but with a higher weight to the words that make up the title in B\n",
        "\n",
        "#intuition:\n",
        "#if I find in A the words that make up the title in B, then there's\n",
        "#a good chance that B is a prerequisite of A\n",
        "\n",
        "#examples of intuition:\n",
        "#the wikipedia page of \"Light\" is a prerequisite of the wikipedia page of \"Total internal reflection\"\n",
        "#the wikipedia page talking about \"Total internal reflection\" mentions the word \"light\" many times\n",
        "#\"Magnet\" is a prerequisite of \"Magnetic field\" \n",
        "#the word \"magnet\" appears various times in the \"Magnetic field\" page\n",
        "\n",
        "#the weight given to the word depends on the length of the document A\n",
        "#the bigger the document the bigger the weight\n",
        "#this is to prevent small documents from being too dependant on the word\n",
        "#and for the word to be irrelevant in big documents\n",
        "\n",
        "word_percent = 0.1\n",
        "\n",
        "#create train dataset\n",
        "\n",
        "ft_train = {k: [] for k in range(600)}\n",
        "ft_train['prerequisite'] = []\n",
        "for index, row in train.iterrows():\n",
        "    A = doc_dict[row[0]]\n",
        "    B_title = row[1]\n",
        "    B = doc_dict[B_title]\n",
        "    doc_embedding_A = np.zeros(300)\n",
        "    doc_embedding_B = np.zeros(300)\n",
        "\n",
        "    #clean the title\n",
        "    B_title = B_title.replace(\"'\",\" \")\n",
        "    #tokenized\n",
        "    B_title_tokenized = nltk.tokenize.word_tokenize(B_title, \"italian\")\n",
        "    #no punctuation and lowercase\n",
        "    B_title_tokenized_no_punct = [x.lower() for x in B_title_tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    B_title = [x for x in B_title_tokenized_no_punct if x not in it_stop_words]\n",
        "\n",
        "    for word in A:\n",
        "      weight = 1\n",
        "      if word in B_title:\n",
        "        weight = 1\n",
        "        weight = len(A) * word_percent\n",
        "      word_embedding = weight * np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_A = doc_embedding_A + word_embedding\n",
        "\n",
        "    for word in B:\n",
        "      weight = 1\n",
        "      if word in B_title:\n",
        "        weight = 1\n",
        "        #weight = len(B) * word_percent\n",
        "      word_embedding = weight * np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_B = doc_embedding_B + word_embedding  \n",
        "    \n",
        "    #averaged doc_embeddings\n",
        "    doc_embedding_A = doc_embedding_A / len(A)\n",
        "    doc_embedding_B = doc_embedding_B / len(B)\n",
        "\n",
        "    data = np.concatenate([doc_embedding_A,doc_embedding_B]) \n",
        "    for i,val in enumerate(data):\n",
        "      ft_train[i].append(val)\n",
        "    ft_train['prerequisite'].append(row[2])\n",
        "\n",
        "#create validation dataset\n",
        "\n",
        "ft_validation = {k: [] for k in range(600)}\n",
        "ft_validation['prerequisite'] = []\n",
        "for index, row in validation.iterrows():\n",
        "    A = doc_dict[row[0]]\n",
        "    B_title = row[1]\n",
        "    B = doc_dict[B_title]\n",
        "    doc_embedding_A = np.zeros(300)\n",
        "    doc_embedding_B = np.zeros(300)\n",
        "\n",
        "    #clean the title\n",
        "    B_title = B_title.replace(\"'\",\" \")\n",
        "    #tokenized\n",
        "    B_title_tokenized = nltk.tokenize.word_tokenize(B_title, \"italian\")\n",
        "    #no punctuation and lowercase\n",
        "    B_title_tokenized_no_punct = [x.lower() for x in B_title_tokenized if x not in punct]\n",
        "    #remove stop words\n",
        "    B_title = [x for x in B_title_tokenized_no_punct if x not in it_stop_words]\n",
        "\n",
        "    for word in A:\n",
        "      weight = 1\n",
        "      if word in B_title:\n",
        "        weight = 1\n",
        "        weight = len(A) * word_percent\n",
        "      word_embedding = weight * np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_A = doc_embedding_A + word_embedding\n",
        "\n",
        "    for word in B:\n",
        "      weight = 1\n",
        "      if word in B_title:\n",
        "        weight = 1\n",
        "        #weight = len(B) * word_percent\n",
        "      word_embedding = weight * np.array(ft.get_word_vector(word))\n",
        "      doc_embedding_B = doc_embedding_B + word_embedding  \n",
        "    \n",
        "    #averaged doc_embeddings\n",
        "    doc_embedding_A = doc_embedding_A / len(A)\n",
        "    doc_embedding_B = doc_embedding_B / len(B)\n",
        "\n",
        "    data = np.concatenate([doc_embedding_A,doc_embedding_B]) \n",
        "    for i,val in enumerate(data):\n",
        "      ft_validation[i].append(val)\n",
        "    ft_validation['prerequisite'].append(row[2])\n",
        "\n",
        "ft_df_train = pd.DataFrame(data = ft_train)\n",
        "ft_df_validation = pd.DataFrame(data = ft_validation)\n",
        "\n",
        "y_train = ft_df_train.iloc[:,600]\n",
        "X_train = ft_df_train.iloc[:,:600]\n",
        "\n",
        "X_test = ft_df_validation.iloc[:,:600]\n",
        "y_test = ft_df_validation.iloc[:,600]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6apDNd2rtrOb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "466fe97b-c248-4b5b-8c17-f2392ff495d8"
      },
      "source": [
        "#Logistic regression\n",
        "\n",
        "#train\n",
        "LR = LogisticRegression(max_iter=1000)\n",
        "LR.fit(X_train, y_train)\n",
        "\n",
        "#validate\n",
        "y_pred = LR.predict(X_test)\n",
        "\n",
        "print('Accuracy score: {:3f}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Precision score: {:3f}'.format(precision_score(y_test, y_pred)))\n",
        "print('Recall score: {:3f}'.format(recall_score(y_test, y_pred)))\n",
        "print('F1 score: {:3f}'.format(f1_score(y_test, y_pred)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Accuracy score: 0.824725'\n",
            "'Precision score: 0.857143'\n",
            "'Recall score: 0.028302'\n",
            "'F1 score: 0.054795'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il-LmMAk_JxR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "b0d92f18-876e-4048-a21d-4c8a5297ddf2"
      },
      "source": [
        "#Linear SVM\n",
        "\n",
        "#train\n",
        "SVM = svm.LinearSVC(max_iter=10000,C=500)\n",
        "SVM.fit(X_train, y_train)\n",
        "\n",
        "#validate\n",
        "y_pred = SVM.predict(X_test)\n",
        "\n",
        "print('Accuracy score: {:3f}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Precision score: {:3f}'.format(precision_score(y_test, y_pred)))\n",
        "print('Recall score: {:3f}'.format(recall_score(y_test, y_pred)))\n",
        "print('F1 score: {:3f}'.format(f1_score(y_test, y_pred)))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Accuracy score: 0.920406'\n",
            "'Precision score: 0.801020'\n",
            "'Recall score: 0.740566'\n",
            "'F1 score: 0.769608'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwI9Z6vgWZ3Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "dbedaf87-dc2c-41d5-d5c0-ece5d52d25c2"
      },
      "source": [
        "#rbf SVM\n",
        "\n",
        "#train\n",
        "SVM = svm.SVC(max_iter=10000,C=500, gamma=10)\n",
        "SVM.fit(X_train, y_train)\n",
        "\n",
        "#validate\n",
        "y_pred = SVM.predict(X_test)\n",
        "\n",
        "print('Accuracy score: {:3f}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Precision score: {:3f}'.format(precision_score(y_test, y_pred)))\n",
        "print('Recall score: {:3f}'.format(recall_score(y_test, y_pred)))\n",
        "print('F1 score: {:3f}'.format(f1_score(y_test, y_pred)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Accuracy score: 0.933108'\n",
            "'Precision score: 0.818182'\n",
            "'Recall score: 0.806604'\n",
            "'F1 score: 0.812352'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdDtCdZVNzoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "49579ea8-9332-43c3-a6ac-05a619de1f78"
      },
      "source": [
        "#Random Forest\n",
        "\n",
        "#train\n",
        "RF = RandomForestClassifier(max_features = None)\n",
        "RF.fit(X_train, y_train)\n",
        "\n",
        "#validate\n",
        "y_pred = RF.predict(X_test)\n",
        "\n",
        "print('Accuracy score: {:3f}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Precision score: {:3f}'.format(precision_score(y_test, y_pred)))\n",
        "print('Recall score: {:3f}'.format(recall_score(y_test, y_pred)))\n",
        "print('F1 score: {:3f}'.format(f1_score(y_test, y_pred)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Accuracy score: 0.932261'\n",
            "'Precision score: 0.892857'\n",
            "'Recall score: 0.707547'\n",
            "'F1 score: 0.789474'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
